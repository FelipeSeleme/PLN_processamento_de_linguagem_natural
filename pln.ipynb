{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desenvolvimento de PLN (Processamento de Linguagem Natural)\n",
    "> Felipe Seleme Ribeiro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação de Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Para esta etapa, será utilizado um cenário fictício de uma empresa que recebe milhares de e-mails de clientes diariamente.  \n",
    "> Os objetivos são:  \n",
    ">  \n",
    "> - Classificar os e-mails automaticamente em categorias, como \"Suporte Técnico\", \"Consulta Comercial\" e \"Reclamação\".\n",
    "> - Gerar respostas automáticas baseadas na categoria detectada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementação de Classificação com BERT:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparando o ambiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É recomendável criar um ambiente virtual e instalar as bibliotecas necessárias:\n",
    "\n",
    "```bash\n",
    "# Criando o ambiente virtual\n",
    "python3 -m venv venv\n",
    "# Ativando o ambiente virtual\n",
    "source venv/bin/activate  # No Windows, use venv\\Scripts\\activate\n",
    "\n",
    "# Instalando as bibliotecas necessárias para esse projeto\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅️ Bibliotecas importadas com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Importação das bibliotecas necessárias\n",
    "import pandas as pd  # Biblioteca para manipulação de dados\n",
    "from sklearn.model_selection import train_test_split  # Biblioteca para divisão de dados\n",
    "from sklearn.preprocessing import LabelEncoder  # Biblioteca para codificação de dados\n",
    "from transformers import BertTokenizer, BertForSequenceClassification  # Biblioteca para tokenização e classificação\n",
    "from torch.optim import AdamW  # Biblioteca para otimização de pesos\n",
    "import torch  # Biblioteca para manipulação de dados\n",
    "import re  # Biblioteca para manipulação de expressões regulares\n",
    "print('✅️ Bibliotecas importadas com sucesso!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurar o dispositivo para utilizar o processamento da GPU se compatível:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅️ Dispositivo configurado como: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('✅️ Dispositivo configurado como:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtenção dos Dados:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definir um dataset de exemplo, contendo **e-mails** e **catagorias**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅️ DataFrame criado com sucesso.\n",
      "Dataframe dos e-mails de exemplo:\n",
      "                                     email           categoria\n",
      "0          Preciso de ajuda com o sistema.     Suporte Técnico\n",
      "1    Gostaria de informações sobre preços.  Consulta Comercial\n",
      "2  Não estou satisfeito com o atendimento.          Reclamação\n",
      "3            O sistema está com problemas.     Suporte Técnico\n",
      "4     Vocês oferecem planos de assinatura?  Consulta Comercial\n"
     ]
    }
   ],
   "source": [
    "# Dataset fictício em estrutura de dicionário Python (tipicamente encontrada em APIs através de arquivos JSON)\n",
    "data = {\n",
    "    \"email\": [\n",
    "        \"Preciso de ajuda com o sistema.\",\n",
    "        \"Gostaria de informações sobre preços.\",\n",
    "        \"Não estou satisfeito com o atendimento.\",\n",
    "        \"O sistema está com problemas.\",\n",
    "        \"Vocês oferecem planos de assinatura?\"\n",
    "    ],\n",
    "    \"categoria\": [\"Suporte Técnico\", \"Consulta Comercial\", \"Reclamação\", \"Suporte Técnico\", \"Consulta Comercial\"]\n",
    "}\n",
    "\n",
    "# Criando o DataFrame com Pandas\n",
    "data_df = pd.DataFrame(data)\n",
    "print('✅️ DataFrame criado com sucesso.')\n",
    "print(\"Dataframe dos e-mails de exemplo:\")\n",
    "print(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpeza e Manipulação dos Dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅️ DataFrame normalizado com sucesso.\n",
      "E-mails normalizados:\n",
      "                                     email  \\\n",
      "0          Preciso de ajuda com o sistema.   \n",
      "1    Gostaria de informações sobre preços.   \n",
      "2  Não estou satisfeito com o atendimento.   \n",
      "3            O sistema está com problemas.   \n",
      "4     Vocês oferecem planos de assinatura?   \n",
      "\n",
      "                            email_cleaned  \n",
      "0          preciso de ajuda com o sistema  \n",
      "1    gostaria de informações sobre preços  \n",
      "2  não estou satisfeito com o atendimento  \n",
      "3            o sistema está com problemas  \n",
      "4     vocês oferecem planos de assinatura  \n"
     ]
    }
   ],
   "source": [
    "# Função para limpar e normalizar o texto\n",
    "def clean_text(text):\n",
    "    \"\"\"Limpa e normaliza o texto.\"\"\"\n",
    "    text = text.lower()  # Converte para minúsculas\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove pontuação\n",
    "    return text\n",
    "\n",
    "# Insere o texto normalizado no Dataframe\n",
    "data_df['email_cleaned'] = data_df['email'].apply(clean_text)\n",
    "\n",
    "print('✅️ DataFrame normalizado com sucesso.')\n",
    "print(\"E-mails normalizados:\")\n",
    "print(data_df[['email', 'email_cleaned']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparação do Modelo para o BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅️ Dados codificados para o modelo BERT.\n"
     ]
    }
   ],
   "source": [
    "# Codificação das categorias (target) - transforma as categorias de texto em números inteiros.\n",
    "label_encoder = LabelEncoder()\n",
    "data_df['label'] = label_encoder.fit_transform(data_df['categoria'])\n",
    "\n",
    "# Tokenização BERT - quebra os textos em sub-palavras e adiciona tokens especiais necessários.\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Divisão dos dados em conjuntos de treinamento (80%) e teste (20%).\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_df['email_cleaned'],  # Os textos processados.\n",
    "    data_df['label'],          # Os rótulos numéricos correspondentes.\n",
    "    test_size=0.2,             # Proporção de dados para o conjunto de teste.\n",
    "    random_state=42            # Para reprodutibilidade.\n",
    ")\n",
    "\n",
    "# Codificação - converte os textos e rótulos em formatos compatíveis com o modelo BERT.\n",
    "def encode_data(texts, labels, tokenizer, max_length=128):\n",
    "    inputs = tokenizer(\n",
    "        texts.to_list(),              # Lista de textos para tokenizar.\n",
    "        padding=True,                 # Adiciona preenchimento nas sequências menores.\n",
    "        truncation=True,              # Trunca sequências maiores que `max_length`.\n",
    "        max_length=max_length,        # Comprimento máximo permitido.\n",
    "        return_tensors=\"pt\"           # Retorna tensores PyTorch.\n",
    "    )\n",
    "    labels = torch.tensor(labels.to_list())  # Converte os rótulos em tensores PyTorch.\n",
    "    return inputs, labels\n",
    "\n",
    "# Aplicando a codificação aos dados de treinamento e teste.\n",
    "train_inputs, train_labels = encode_data(X_train, y_train, tokenizer)\n",
    "test_inputs, test_labels = encode_data(X_test, y_test, tokenizer)\n",
    "\n",
    "# Inicialização do modelo BERT\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",  # Modelo BERT pré-treinado.\n",
    "    num_labels=3          # Número de categorias de saída.\n",
    ")\n",
    "model.to(device)  # Move o modelo para o dispositivo (CPU ou GPU).\n",
    "# Configuração do otimizador\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)  # Taxa de aprendizado padrão recomendada para BERT.\n",
    "\n",
    "print(\"✅️ Dados codificados para o modelo BERT.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento (simplificado - exemplo apenas com 2 epocas):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.9944085478782654\n",
      "Epoch 2, Loss: 1.0881786346435547\n"
     ]
    }
   ],
   "source": [
    "model.train()  # Coloca o modelo em modo de treinamento\n",
    "for epoch in range(2):  # Loop de treinamento por 2 épocas\n",
    "    inputs = {key: val.to(device) for key, val in train_inputs.items()}  # Move as entradas (inputs) para o dispositivo (CPU ou GPU)\n",
    "    labels = train_labels.to(device)   # Move os rótulos (labels) para o dispositivo (GPU ou CPU)\n",
    "\n",
    "    optimizer.zero_grad()   # Zera os gradientes acumulados das iterações anteriores\n",
    "    outputs = model(**inputs, labels=labels)  # Passa as entradas pelo modelo para calcular as previsões\n",
    "    loss = outputs.loss  # A perda é extraída da saída do modelo\n",
    "    loss.backward()   # Calcula os gradientes em relação à perda\n",
    "    optimizer.step()  # Atualiza os pesos do modelo\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O valor da perda diminui da primeira para a segunda época. O que indica que o modelo está aprendendo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teste de Classificação do Modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categoria Prevista: Suporte Técnico\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Coloca o modelo em modo de avaliação\n",
    "\n",
    "# Novo email que será classificado\n",
    "new_email = \"Preciso de suporte com um problema no sistema.\"\n",
    "\n",
    "# Limpeza do texto para garantir que o novo email passe pelo mesmo pré-processamento que os dados de treinamento.\n",
    "new_email_cleaned = clean_text(new_email)\n",
    "\n",
    "# Tokeniza o novo email\n",
    "new_input = tokenizer(new_email_cleaned, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "# Desativa o cálculo de gradientes para acelerar a inferência e economizar memória.\n",
    "with torch.no_grad():\n",
    "    output = model(**new_input)  # Passa o texto tokenizado pelo modelo\n",
    "    predicted = torch.argmax(output.logits, dim=1).item()  # Obtém a categoria prevista\n",
    "    categoria = label_encoder.inverse_transform([predicted])[0]   # Converte a classe prevista de volta para seu valor original de categoria\n",
    "    print(f\"Categoria Prevista: {categoria}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sucesso! O modelo fez uma categorização correta do e-mail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manipulação de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerando o seguinte conjunto de dados textuais:\n",
    "- O gato está no telhado.\n",
    "- A chuva cai sem parar.\n",
    "- Gosto de assistir filmes nos finais de semana.\n",
    "- Ele está estudando para as provas finais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅️ Textos carregados com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# Conjunto de dados textuais\n",
    "texts = [\n",
    "    \"O gato está no telhado.\",\n",
    "    \"A chuva cai sem parar.\",\n",
    "    \"Gosto de assistir filmes nos finais de semana.\",\n",
    "    \"Ele está estudando para as provas finais.\"\n",
    "]\n",
    "print('✅️ Textos carregados com sucesso.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizar a limpeza dos textos, incluindo a remoção de stopwords, normalização (caixa baixa), e tokenização."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para essa etapa, foi escolhida a biblioteca **NLTK**.  \n",
    "Importando bibliotecas adicionais necessárias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅️ Bibliotecas adicionais importadas com sucesso.\n"
     ]
    }
   ],
   "source": [
    "import nltk  # Biblioteca para processamento de linguagem natural\n",
    "from nltk.corpus import stopwords  # Conjunto de stopwords (como \"e\", \"o\", \"a\", \"de\") que não são úteis em tarefas de análise de texto\n",
    "from nltk.tokenize import word_tokenize  # Divide o texto em palavras ou tokens\n",
    "from nltk.stem import WordNetLemmatizer  # Reduz as palavras às suas formas base (como \"correndo\" para \"correr\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer  # TF-IDF e Bag of Words (BoW) explicados mais abaixo.\n",
    "print('✅️ Bibliotecas adicionais importadas com sucesso.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/felipeselemeribeiro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/felipeselemeribeiro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baixar pacotes necessários do NLTK\n",
    "nltk.download('punkt')  # Baixa os pacotes necessários para tokenização.\n",
    "nltk.download('stopwords')  # Baixa o pacote de stopwords.\n",
    "\n",
    "# nltk.download('all', force=True)  # Baixa todos os pacotes. Descomente para garantir a execução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅️ Stopwords removidos. Texto tokenizado.\n",
      "Texto original: O gato está no telhado.\n",
      "Texto limpo e tokenizado: ['gato', 'telhado'] \n",
      "\n",
      "Texto original: A chuva cai sem parar.\n",
      "Texto limpo e tokenizado: ['chuva', 'cai', 'parar'] \n",
      "\n",
      "Texto original: Gosto de assistir filmes nos finais de semana.\n",
      "Texto limpo e tokenizado: ['gosto', 'assistir', 'filmes', 'finais', 'semana'] \n",
      "\n",
      "Texto original: Ele está estudando para as provas finais.\n",
      "Texto limpo e tokenizado: ['estudando', 'provas', 'finais'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remoção de stopwords\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "# Normalização (caixa baixa) e Tokenização\n",
    "def clean_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Converte o texto para minúsculas e tokeniza.\n",
    "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]  # Remove stopwords e palavras não alfanuméricas.\n",
    "    return tokens\n",
    "print('✅️ Stopwords removidos. Texto tokenizado.')\n",
    "\n",
    "# Limpeza de cada texto na lista\n",
    "norm_texts = [clean_text(text) for text in texts]\n",
    "\n",
    "# Exibe os resultados\n",
    "for original, cleaned in zip(texts, norm_texts):\n",
    "    print(\"Texto original:\", original)\n",
    "    print(\"Texto limpo e tokenizado:\", cleaned, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os stopwords foram removidos e o texto foi tokenizado com sucesso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando lematização aos textos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para essa etapa, utilizaremos a biblioteca **SpaCy**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Biblioteca SpaCy importada com sucesso.\n",
      "Collecting pt-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.8.0/pt_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.0/13.0 MB 59.0 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy  # Biblioteca SpaCy para processamento de linguagem natural\n",
    "print(\"✅ Biblioteca SpaCy importada com sucesso.\")\n",
    "\n",
    "# Baixar o modelo de português do SpaCy\n",
    "import os\n",
    "if not os.path.exists(\"pt_core_news_sm\"):\n",
    "    os.system(\"python -m spacy download pt_core_news_sm\")\n",
    "\n",
    "# Carregar o modelo de português\n",
    "nlp = spacy.load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original: O gato está no telhado.\n",
      "Texto limpo e lematizado: ['gato', 'telhado'] \n",
      "\n",
      "Texto original: A chuva cai sem parar.\n",
      "Texto limpo e lematizado: ['chuva', 'cair', 'parar'] \n",
      "\n",
      "Texto original: Gosto de assistir filmes nos finais de semana.\n",
      "Texto limpo e lematizado: ['gostar', 'assistir', 'filme', 'final', 'semana'] \n",
      "\n",
      "Texto original: Ele está estudando para as provas finais.\n",
      "Texto limpo e lematizado: ['estudar', 'prova', 'final'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Função para limpeza e lematização usando SpaCy\n",
    "def clean_and_lemmatize_spacy(text):\n",
    "    doc = nlp(text.lower())  # Processa o texto e converte para caixa baixa\n",
    "    tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]  # Lematiza, remove stopwords e não alfanuméricos\n",
    "    return tokens\n",
    "\n",
    "# Aplica a lematização com SpaCy em cada texto\n",
    "lemmatized_texts = [clean_and_lemmatize_spacy(text) for text in texts]\n",
    "\n",
    "# Exibe os resultados\n",
    "for original, cleaned in zip(texts, lemmatized_texts):\n",
    "    print(\"Texto original:\", original)\n",
    "    print(\"Texto limpo e lematizado:\", cleaned, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando vetores de característica utilizando TF-IDF e Bag of Words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['gato', 'telhado'], ['chuva', 'cair', 'parar'], ['gostar', 'assistir', 'filme', 'final', 'semana'], ['estudar', 'prova', 'final']]\n"
     ]
    }
   ],
   "source": [
    "# Imprime os textos limpos, tokenizados e lematizados\n",
    "print(lemmatized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gato telhado', 'chuva cair parar', 'gostar assistir filme final semana', 'estudar prova final']\n"
     ]
    }
   ],
   "source": [
    "# Converter listas de tokens para strings\n",
    "lemmatized_strings = [\" \".join(tokens) for tokens in lemmatized_texts]\n",
    "print(lemmatized_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF - Cria vetores de características com base na frequência relativa e na importância das palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz TF-IDF:\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.70710678 0.         0.         0.         0.         0.70710678]\n",
      " [0.         0.57735027 0.57735027 0.         0.         0.\n",
      "  0.         0.         0.57735027 0.         0.         0.        ]\n",
      " [0.46516193 0.         0.         0.         0.46516193 0.36673901\n",
      "  0.         0.46516193 0.         0.         0.46516193 0.        ]\n",
      " [0.         0.         0.         0.61761437 0.         0.48693426\n",
      "  0.         0.         0.         0.61761437 0.         0.        ]]\n",
      "Vocabulário TF-IDF: ['assistir' 'cair' 'chuva' 'estudar' 'filme' 'final' 'gato' 'gostar'\n",
      " 'parar' 'prova' 'semana' 'telhado']\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(lemmatized_strings)\n",
    "\n",
    "print(\"Matriz TF-IDF:\")\n",
    "print(tfidf_vectors.toarray())\n",
    "print(\"Vocabulário TF-IDF:\", tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words (BoW) - Constrói vetores de características considerando apenas a contagem de palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz Bag of Words:\n",
      "[[0 0 0 0 0 0 1 0 0 0 0 1]\n",
      " [0 1 1 0 0 0 0 0 1 0 0 0]\n",
      " [1 0 0 0 1 1 0 1 0 0 1 0]\n",
      " [0 0 0 1 0 1 0 0 0 1 0 0]]\n",
      "Vocabulário Bag of Words: ['assistir' 'cair' 'chuva' 'estudar' 'filme' 'final' 'gato' 'gostar'\n",
      " 'parar' 'prova' 'semana' 'telhado']\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words (BoW)\n",
    "bow_vectorizer = CountVectorizer()\n",
    "bow_vectors = bow_vectorizer.fit_transform(lemmatized_strings)\n",
    "\n",
    "print(\"\\nMatriz Bag of Words:\")\n",
    "print(bow_vectors.toarray())\n",
    "print(\"Vocabulário Bag of Words:\", bow_vectorizer.get_feature_names_out())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
